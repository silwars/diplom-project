{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e9432-99eb-448c-9b44-4518e4bf8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Параметры, которые можно настроить:\n",
    "csv_path = input(\"Введите путь к CSV-файлу CICIDS2017: \").strip()\n",
    "min_support = 10  # минимальная поддержка (количество транзакций), примерное значение\n",
    "num_bins = 5      # количество интервалов для дискретизации числовых признаков\n",
    "chunk_size = 10000 # размер чанка при потоковой загрузке CSV\n",
    "\n",
    "# Определяем названия столбцов, которые нужно дискретизировать (числовые признаки)\n",
    "numeric_columns = ['Flow Duration' , 'Active Mean' ,'Idle Mean' , 'Average Packet Size', 'Total Fwd Packets', 'Total Length of Fwd Packets' ]\n",
    "# Примечание: Список numeric_columns можно расширить или скорректировать под фактические столбцы набора данных.\n",
    "# Определяем столбцы, которые являются уже категориальными (в том числе Label)\n",
    "categorical_columns = ['FIN Flag Count','SYN Flag Count','RST Flag Count','PSH Flag Count','ACK Flag Count'] #'\n",
    "# Примечание: \"Protocol\" может быть числовым кодом, но мы будем считать его категориальным (например, Protocol=6 как TCP).\n",
    "# \"Label\" - метка атаки, уже категориальная.\n",
    "\n",
    "# Словари для сбора глобального min и max по числовым столбцам\n",
    "min_values = {col: float('inf') for col in numeric_columns}\n",
    "max_values = {col: float('-inf') for col in numeric_columns}\n",
    "\n",
    "# Первый проход: находим глобальные min и max для числовых признаков\n",
    "for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n",
    "    # Обновляем минимумы и максимумы\n",
    "    for col in numeric_columns:\n",
    "        if col in chunk.columns:\n",
    "            col_min = chunk[col].min(skipna=True)\n",
    "            col_max = chunk[col].max(skipna=True)\n",
    "            # Обновляем глобальные значения\n",
    "            if pd.notna(col_min):\n",
    "                if col_min < min_values[col]:\n",
    "                    min_values[col] = col_min\n",
    "            if pd.notna(col_max):\n",
    "                if col_max > max_values[col]:\n",
    "                    max_values[col] = col_max\n",
    "\n",
    "# Определяем интервалы (bins) для каждого числового столбца\n",
    "bin_edges = {}\n",
    "for col in numeric_columns:\n",
    "    if min_values[col] == float('inf'):\n",
    "        # Если столбца нет в данных или не найден, пропускаем\n",
    "        continue\n",
    "    # Добавляем небольшой эпсилон к max чтобы включить правый край в последний интервал\n",
    "    min_val = min_values[col]\n",
    "    max_val = max_values[col]\n",
    "    if min_val >= max_val:\n",
    "        # Если значение постоянно, можно создать одну категорию\n",
    "        bin_edges[col] = [min_val]\n",
    "    else:\n",
    "        # Равномерные интервалы\n",
    "        bin_edges[col] = [min_val + i * (max_val - min_val) / num_bins for i in range(1, num_bins)] \n",
    "        # (Будет num_bins-1 внутренних границ; pandas.cut сам добавит min и max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d7972-0591-4342-a33a-b88688a52658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Структуры для подсчета поддержки\n",
    "item_support = defaultdict(int)  # частота каждого элемента\n",
    "pair_support = defaultdict(int)  # частота каждой пары элементов\n",
    "\n",
    "# Список транзакций (можем записывать во временный файл, но для простоты собираем список, \n",
    "# при очень больших данных стоит обрабатывать по частям)\n",
    "transactions = []\n",
    "\n",
    "for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n",
    "    for _, row in chunk.iterrows():\n",
    "        transaction_items = []\n",
    "        # Преобразуем числовые признаки в категориальные по бинам\n",
    "        for col in numeric_columns:\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                val = row[col]\n",
    "                # Находим бин (категорию) для значения\n",
    "                if min_values[col] == float('inf'):\n",
    "                    continue  # столбец отсутствует\n",
    "                if min_values[col] >= max_values[col]:\n",
    "                    # Константное значение, используем как есть\n",
    "                    cat_label = f\"{col}={val}\"\n",
    "                else:\n",
    "                    # Определяем интервал через pd.cut (разовое использование с нашими границами)\n",
    "                    bins = [min_values[col]] + bin_edges[col] + [max_values[col] + 1e-9]  # + эпсилон к последнему\n",
    "                    # Найдём индекс интервала, в который попадает значение\n",
    "                    # Пройдём по границам, чтобы определить категорию\n",
    "                    cat_idx = None\n",
    "                    for i in range(len(bins)-1):\n",
    "                        if bins[i] <= val < bins[i+1]:\n",
    "                            cat_idx = i\n",
    "                            break\n",
    "                    if cat_idx is None:\n",
    "                        cat_idx = len(bins) - 2  # если значение == max_value\n",
    "                    cat_label = f\"{col}=Bin{cat_idx}\"\n",
    "                transaction_items.append(cat_label)\n",
    "        # Обрабатываем категориальные признаки\n",
    "        for col in categorical_columns:\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                val = row[col]\n",
    "                transaction_items.append(f\"{col}={val}\")\n",
    "        # Обновляем поддержку для каждого элемента и каждой пары\n",
    "        # Удаляем дубликаты в транзакции (если таковые есть), т.к. для алгоритма ассоциации важна только присутствие\n",
    "        transaction_items = list(set(transaction_items))\n",
    "        # Пропускаем пустые транзакции\n",
    "        if not transaction_items:\n",
    "            continue\n",
    "        # Учитываем поддержку элементов\n",
    "        for item in transaction_items:\n",
    "            item_support[item] += 1\n",
    "        # Учитываем поддержку пар элементов (совстречаемость)\n",
    "        # Используем itertools.combinations для генерации всех пар (без учета порядка)\n",
    "        for item_a, item_b in combinations(sorted(transaction_items), 2):\n",
    "            pair_support[(item_a, item_b)] += 1\n",
    "        # Сохраняем транзакцию во временный список\n",
    "        transactions.append(transaction_items)\n",
    "\n",
    "# Фильтрация редких элементов по порогу поддержки\n",
    "frequent_items = {item for item, sup in item_support.items() if sup >= min_support}\n",
    "# Если элемент не частый, удаляем его из всех транзакций и не учитываем в дальнейшем\n",
    "filtered_transactions = []\n",
    "for items in transactions:\n",
    "    # Оставляем только частые элементы\n",
    "    freq_items = [item for item in items if item in frequent_items]\n",
    "    if freq_items:\n",
    "        filtered_transactions.append(freq_items)\n",
    "transactions = filtered_transactions  # заменяем исходные транзакции отфильтрованными\n",
    "\n",
    "# Освобождаем память, удаляя вспомогательные структуры, если они больше не нужны\n",
    "del filtered_transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71dcbc-f742-4cb2-8a3f-319b519aea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для узлов FP-дерева\n",
    "class FPNode:\n",
    "    __slots__ = (\"item\", \"count\", \"parent\", \"children\", \"link\", \"array\", \"leaf\")\n",
    "    def __init__(self, item, parent):\n",
    "        self.item = item       # Название элемента (None для корня)\n",
    "        self.count = 1         # Инициализируем счетчик\n",
    "        self.parent = parent   # Ссылка на родительский узел\n",
    "        self.children = {}     # Словарь: элемент -> дочерний узел\n",
    "        self.link = None       # Ссылка на следующий узел такого же элемента\n",
    "        self.array = 0         # Маркер использования \"array\" (будет 1 для узлов разреженных элементов)\n",
    "        self.leaf = True       # Лист (если появятся дети, переключим на False)\n",
    "\n",
    "# Строим FP-дерево\n",
    "header_table = {}  # Таблица заголовков: элемент -> первый узел в цепочке данного элемента\n",
    "root = FPNode(None, None)  # корневой узел (item=None)\n",
    "\n",
    "# Получаем порядок сортировки элементов по частоте (убывание) из частотной таблицы\n",
    "item_order = sorted(item_support.keys(), key=lambda x: item_support[x], reverse=True)\n",
    "\n",
    "for transaction in transactions:\n",
    "    # Сортируем элементы транзакции по глобальной частоте (убывание)\n",
    "    sorted_items = sorted(transaction, key=lambda x: item_support[x], reverse=True)\n",
    "    # Вставляем по порядку в дерево\n",
    "    current_node = root\n",
    "    for item in sorted_items:\n",
    "        # Если дочерний узел с таким item существует - увеличиваем счетчик\n",
    "        if item in current_node.children:\n",
    "            child = current_node.children[item]\n",
    "            child.count += 1\n",
    "        else:\n",
    "            # Создаем новый узел\n",
    "            child = FPNode(item, current_node)\n",
    "            current_node.children[item] = child\n",
    "            # Добавляем узел в таблицу заголовков\n",
    "            if item in header_table:\n",
    "                # Найдем последний узел в цепочке link и добавим новый\n",
    "                last_node = header_table[item]\n",
    "                while last_node.link is not None:\n",
    "                    last_node = last_node.link\n",
    "                last_node.link = child\n",
    "            else:\n",
    "                header_table[item] = child\n",
    "        # При добавлении нового узла у родительского узла уже не лист\n",
    "        current_node.leaf = False\n",
    "        # Переходим на уровень ниже\n",
    "        current_node = child\n",
    "\n",
    "# После построения дерева, помечаем leaf для конечных узлов явно (для всех узлов, которые остались листами)\n",
    "# (Уже учтено: мы устанавливали leaf=False при добавлении детей)\n",
    "\n",
    "# Определяем порог для разреженных элементов (например, threshold = 5 узлов)\n",
    "sparse_threshold = 5\n",
    "\n",
    "# Вычисляем число узлов для каждого элемента по цепочке link\n",
    "item_node_count = {}\n",
    "for item, node in header_table.items():\n",
    "    count = 0\n",
    "    current = node\n",
    "    while current is not None:\n",
    "        count += 1\n",
    "        current = current.link\n",
    "    item_node_count[item] = count\n",
    "\n",
    "# Отмечаем узлы: если элемент разреженный (узлов >= threshold), устанавливаем node.array = 1 для всех его узлов\n",
    "sparse_items = {item for item, cnt in item_node_count.items() if cnt >= sparse_threshold}\n",
    "dense_items = set(item_node_count.keys()) - sparse_items\n",
    "\n",
    "for item in sparse_items:\n",
    "    node = header_table.get(item)\n",
    "    while node is not None:\n",
    "        node.array = 1\n",
    "        node = node.link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33816a-a935-4970-ba6a-db717136f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Максимальный размер наборов\n",
    "max_itemset_size = 7\n",
    "\n",
    "# Словарь для хранения всех частых наборов (ключ – frozenset набора, значение – поддержка)\n",
    "frequent_itemsets = {}\n",
    "\n",
    "# -------------------------------\n",
    "# Шаг 1. Частые 1-элементные наборы\n",
    "# -------------------------------\n",
    "L1 = set()\n",
    "for item, sup in item_support.items():\n",
    "    if sup >= min_support:\n",
    "        fs_item = frozenset([item])\n",
    "        L1.add(fs_item)\n",
    "        frequent_itemsets[fs_item] = sup\n",
    "\n",
    "# -------------------------------\n",
    "# Шаг 2. Частые 2-элементные наборы\n",
    "# Используем sparse_items для генерации пар\n",
    "# -------------------------------\n",
    "L2 = set()\n",
    "for item in sparse_items:\n",
    "    for other, sup in item_support.items():\n",
    "        if other == item:\n",
    "            continue\n",
    "        candidate = frozenset([item, other])\n",
    "        # Упорядочим пару для проверки в pair_support\n",
    "        pair = tuple(sorted(candidate))\n",
    "        if pair in pair_support and pair_support[pair] >= min_support:\n",
    "            L2.add(candidate)\n",
    "            frequent_itemsets[candidate] = pair_support[pair]\n",
    "\n",
    "# Сохраняем уровневые частые наборы в список\n",
    "# levels[0] соответствует 1-элементным, levels[1] – 2-элементным наборам\n",
    "levels = []\n",
    "levels.append(L1)\n",
    "levels.append(L2)\n",
    "\n",
    "# -------------------------------\n",
    "# Шаг 3. Генерация кандидатов для наборов размера 3 до max_itemset_size\n",
    "# -------------------------------\n",
    "for k in range(3, max_itemset_size + 1):\n",
    "    prev_level = list(levels[-1])\n",
    "    candidate_itemsets = set()\n",
    "    \n",
    "    # Объединяем два набора предыдущего уровня, если они имеют k-2 общих элементов\n",
    "    for i in range(len(prev_level)):\n",
    "        for j in range(i + 1, len(prev_level)):\n",
    "            candidate = prev_level[i] | prev_level[j]\n",
    "            if len(candidate) == k:\n",
    "                # Проверка Apriori: все (k-1)-поднаборы кандидата должны быть частыми\n",
    "                all_subsets_frequent = True\n",
    "                for subset in itertools.combinations(candidate, k - 1):\n",
    "                    if frozenset(subset) not in levels[-1]:\n",
    "                        all_subsets_frequent = False\n",
    "                        break\n",
    "                if all_subsets_frequent:\n",
    "                    candidate_itemsets.add(candidate)\n",
    "                    \n",
    "    # Подсчёт поддержки кандидатов, используя список транзакций\n",
    "    current_level = set()\n",
    "    for candidate in candidate_itemsets:\n",
    "        support_count = 0\n",
    "        for trans in transactions:\n",
    "            if candidate.issubset(trans):\n",
    "                support_count += 1\n",
    "        if support_count >= min_support:\n",
    "            current_level.add(candidate)\n",
    "            frequent_itemsets[candidate] = support_count\n",
    "            \n",
    "    # Если не найдено ни одного частого набора, прекращаем дальнейшее расширение\n",
    "    if not current_level:\n",
    "        break\n",
    "    levels.append(current_level)\n",
    "\n",
    "# Теперь frequent_itemsets содержит все частые наборы от 1 до 7 элементов (или меньше, если на каком-то уровне наборов не нашлось)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281ea33-e54e-4095-974f-40d06a75588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рекурсивный обход FP-дерева для извлечения частых наборов плотных элементов\n",
    "frequent_paths = []  # для хранения найденных путей (наборов) с их поддержкой\n",
    "\n",
    "def dfs(node, current_path):\n",
    "    # Обходим каждого дочернего узла\n",
    "    for item, child in node.children.items():\n",
    "        # Если встречаем узел разреженного элемента, пропускаем его ветвь\n",
    "        if child.array == 1:\n",
    "            continue  # пропускаем все подветви этого узла (как указано в алгоритме для плотной обработки)\n",
    "        # Добавляем элемент узла в текущий путь\n",
    "        new_path = current_path + [child.item]\n",
    "        # Если узел является листом или далее по ветви только разреженные, фиксируем путь\n",
    "        if child.leaf or not any(grandchild.array == 0 for grandchild in child.children.values()):\n",
    "            # Вычисляем поддержку для набора new_path.\n",
    "            # Поддержка набора элементов = счетчик этого узла (так как в FP-дереве count узла = количество транзакций,\n",
    "            # содержащих путь от корня до данного узла).\n",
    "            support_count = child.count\n",
    "            frequent_paths.append((frozenset(new_path), support_count))\n",
    "        # Рекурсивно спускаемся глубже\n",
    "        dfs(child, new_path)\n",
    "    # (Возврат обратно не требуется, так как мы передаём копии списка current_path при углублении)\n",
    "\n",
    "# Запускаем DFS от корня (корень не содержит элемента)\n",
    "dfs(root, [])\n",
    "\n",
    "# Добавляем все полученные частые наборы плотных элементов в словарь frequent_itemsets\n",
    "for itemset, sup in frequent_paths:\n",
    "    # Если набор уже есть (возможно, он найден и другим методом) и поддержка совпадает, можно пропустить, \n",
    "    # но на всякий случай мы можем обновить на наибольшую поддержку.\n",
    "    if itemset in frequent_itemsets:\n",
    "        existing_sup = frequent_itemsets[itemset]\n",
    "        if sup > existing_sup:\n",
    "            frequent_itemsets[itemset] = sup\n",
    "    else:\n",
    "        if sup >= min_support:\n",
    "            frequent_itemsets[itemset] = sup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb6836-c276-4319-984c-9a4f69ebec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Группируем частые наборы по их размеру (длине)\n",
    "sd_structure = defaultdict(list)\n",
    "for itemset in frequent_itemsets.keys():\n",
    "    length = len(itemset)\n",
    "    sd_structure[length].append(itemset)\n",
    "\n",
    "# Сортируем ключи (размеры) по возрастанию\n",
    "sorted_lengths = sorted(sd_structure.keys())\n",
    "\n",
    "# Подготавливаем данные для записи: превращаем каждый набор в строку\n",
    "sd_rows = []\n",
    "for length in sorted_lengths:\n",
    "    patterns = sd_structure[length]\n",
    "    # Преобразуем каждый набор во строковой формат, например \"item1 & item2 & item3\"\n",
    "    pattern_strings = []\n",
    "    for itemset in patterns:\n",
    "        items = sorted(itemset)\n",
    "        pattern_strings.append(\"{\" + \", \".join(items) + \"}\")\n",
    "    # Объединяем все паттерны данной длины в одну строку разделённую '; '\n",
    "    patterns_joined = \"; \".join(pattern_strings)\n",
    "    sd_rows.append([length, patterns_joined])\n",
    "\n",
    "# Сохраняем структуру SD в CSV файл\n",
    "with open(\"sd_structure.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Length\", \"Patterns\"])\n",
    "    writer.writerows(sd_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f2e1f-d8b2-4a41-bfdc-7741e0b7714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры для правил\n",
    "min_confidence = 0.9\n",
    "min_lift = 1.5\n",
    "num_transactions = len(transactions)\n",
    "\n",
    "rules = []\n",
    "for itemset, sup in frequent_itemsets.items():\n",
    "    if len(itemset) < 2:\n",
    "        continue  # пропускаем одноэлементные наборы, из них правил не сделать\n",
    "    # Рассматриваем все разбиения itemset на A -> B\n",
    "    # Для избежания дублирования, можем например перебирать все непустые A, где A не больше половины набора (но тогда и B будет повторяться).\n",
    "    # Проще: перебирать все непустые подмножества A (кроме полного набора), тогда B определяется однозначно.\n",
    "    items = list(itemset)\n",
    "    n = len(items)\n",
    "    # Перебираем размеры A от 1 до n-1\n",
    "    for r in range(1, n):\n",
    "        from itertools import combinations\n",
    "        for combo in combinations(items, r):\n",
    "            A = set(combo)\n",
    "            B = itemset - A\n",
    "            if not B:\n",
    "                continue\n",
    "            # Поддержка A и B\n",
    "            support_A = frequent_itemsets.get(frozenset(A), None)\n",
    "            support_B = frequent_itemsets.get(frozenset(B), None)\n",
    "            # Если по каким-то причинам нет поддержки A или B в словаре (например, B не встречался как частый сам по себе),\n",
    "            # то вычислим вручную, пробежав по транзакциям. Но чаще всего frequent_itemsets будет содержать их, особенно A, \n",
    "            # так как A может быть не частым по min_support, однако правило тогда не пройдет по confidence.\n",
    "            if support_A is None:\n",
    "                # Подсчет поддержки A путем прохода (медленно, но редко нужно)\n",
    "                support_A = 0\n",
    "                for trans in transactions:\n",
    "                    if A.issubset(trans):\n",
    "                        support_A += 1\n",
    "            if support_B is None:\n",
    "                support_B = 0\n",
    "                for trans in transactions:\n",
    "                    if B.issubset(trans):\n",
    "                        support_B += 1\n",
    "            # Вычисляем confidence и lift\n",
    "            conf = sup / support_A if support_A > 0 else 0\n",
    "            lift = (conf / (support_B / num_transactions)) if support_B and num_transactions else float('inf')\n",
    "            if conf >= min_confidence and lift >= min_lift:\n",
    "                rules.append((A, B, sup, conf, lift))\n",
    "\n",
    "# Сохраняем правила в CSV\n",
    "with open(\"association_rules.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Antecedent\", \"Consequent\", \"Support\", \"Confidence\", \"Lift\"])\n",
    "    for A, B, sup, conf, lift in rules:\n",
    "        A_str = \"{\" + \", \".join(sorted(A)) + \"}\"\n",
    "        B_str = \"{\" + \", \".join(sorted(B)) + \"}\"\n",
    "        writer.writerow([A_str, B_str, sup, f\"{conf:.2f}\", f\"{lift:.2f}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74c600-3419-450e-b25b-8c636d77938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для frequent_itemsets.csv\n",
    "freq_rows = []\n",
    "for itemset, sup in frequent_itemsets.items():\n",
    "    items = sorted(itemset)\n",
    "    itemset_str = \"{\" + \", \".join(items) + \"}\"\n",
    "    freq_rows.append((len(itemset), itemset_str, sup))\n",
    "\n",
    "# Отсортируем по размеру набора, затем по поддержке (убывание) например\n",
    "freq_rows.sort(key=lambda x: (x[0], -x[2], x[1]))\n",
    "\n",
    "# Запись в CSV\n",
    "with open(\"frequent_itemsets.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Itemset\", \"Support\"])\n",
    "    for _, itemset_str, sup in freq_rows:\n",
    "        writer.writerow([itemset_str, sup])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41dcd47-b9b5-44e1-a6e2-1d7b90197805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
