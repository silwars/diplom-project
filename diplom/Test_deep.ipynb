{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499edf2e-edf0-4d95-8b1e-e7ff83333d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее число транзакций: 56614\n",
      "Найдено 1340028 частых наборов\n",
      "Частые наборы сохранены в frequent_itemsets.csv\n",
      "Найдено 616624 ассоциативных правил с min_conf = 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Исправленный и оптимизированный вариант кода для генерации ассоциативных правил (a) -> label.\n",
    "Реализация опирается на:\n",
    "  - Предобработку: дискретизацию числовых признаков и отбор ключевых столбцов.\n",
    "  - Преобразование данных в транзакции с кодированием айтемов через Pandas category.\n",
    "  - Построение FP-дерева с оптимизированным обновлением (с использованием счётчиков, без дублирования списков).\n",
    "  - Майнинг частых наборов (FP-growth) и генерация правил, где правая часть – это Label.\n",
    "  - Вывод всех найденных правил и сохранение результатов в CSV файлы.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain, combinations\n",
    "\n",
    "##############################################\n",
    "# 1. Предобработка данных: дискретизация и отбор признаков\n",
    "##############################################\n",
    "\n",
    "def preprocess_data(df, selected_columns):\n",
    "    \"\"\"\n",
    "    Выполняет предварительную обработку DataFrame:\n",
    "    - Отбирает указанные столбцы.\n",
    "    - Для числовых столбцов выполняет дискретизацию (пример: деление на 3 категории).\n",
    "    - Преобразует значения в строковые представления вида \"Column:Value\".\n",
    "    Возвращает DataFrame с отобранными/дискретизированными столбцами.\n",
    "    \"\"\"\n",
    "    # Пример дискретизации для непрерывных признаков.\n",
    "    if 'Flow Duration' in df.columns:\n",
    "        df['Flow Duration_cat'] = pd.cut(df['Flow Duration'],\n",
    "                                         bins=[0, 1e5, 1e6, float('inf')],\n",
    "                                         labels=['short', 'medium', 'long'])\n",
    "    if 'Flow Bytes/s' in df.columns:\n",
    "        # Замена бесконечностей и NaN на 0, затем дискретизация.\n",
    "        df['Flow Bytes/s'] = df['Flow Bytes/s'].replace([float('inf'), np.nan], 0)\n",
    "        df['Flow Bytes/s_cat'] = pd.cut(df['Flow Bytes/s'],\n",
    "                                        bins=[-0.1, 1e5, 1e6, float('inf')],\n",
    "                                        labels=['low', 'mid', 'high'])\n",
    "    # Формирование нового списка столбцов.\n",
    "    new_cols = []\n",
    "    for col in selected_columns:\n",
    "        if col in ['Flow Duration', 'Flow Bytes/s']:\n",
    "            if col == 'Flow Duration' and 'Flow Duration_cat' in df.columns:\n",
    "                new_cols.append('Flow Duration_cat')\n",
    "            elif col == 'Flow Bytes/s' and 'Flow Bytes/s_cat' in df.columns:\n",
    "                new_cols.append('Flow Bytes/s_cat')\n",
    "        else:\n",
    "            new_cols.append(col)\n",
    "    df_proc = df[new_cols].copy()\n",
    "    # Преобразуем значения каждого столбца к формату \"Column:Value\"\n",
    "    for col in df_proc.columns:\n",
    "        df_proc[col] = df_proc[col].astype('category')\n",
    "        df_proc[col] = df_proc[col].apply(lambda x: f\"{col}:{x}\")\n",
    "    return df_proc\n",
    "##############################################\n",
    "# 2. Преобразование данных в транзакции с кодированием айтемов\n",
    "##############################################\n",
    "\n",
    "def transactions_from_df(df):\n",
    "    \"\"\"\n",
    "    Преобразует DataFrame в список транзакций,\n",
    "    где каждая транзакция – это список строк вида \"Column:Value\".\n",
    "    \"\"\"\n",
    "    return df.values.tolist()\n",
    "\n",
    "##############################################\n",
    "# 3. Реализация FP-дерева (оптимизированная версия)\n",
    "##############################################\n",
    "\n",
    "class FPNode:\n",
    "    def __init__(self, item, count, parent):\n",
    "        self.item = item          # Имя элемента\n",
    "        self.count = count        # Счётчик\n",
    "        self.parent = parent      # Родительский узел\n",
    "        self.children = {}        # Дочерние узлы: {item: FPNode}\n",
    "        self.node_link = None     # Ссылка для объединения узлов с одинаковым именем\n",
    "\n",
    "    def increment(self, count):\n",
    "        self.count += count\n",
    "def update_tree(items, node, header_table, count):\n",
    "    \"\"\"\n",
    "    Рекурсивно обновляет FP-дерево.\n",
    "    items: отсортированный список элементов транзакции.\n",
    "    node: текущий узел FP-дерева.\n",
    "    header_table: таблица заголовков, где для каждого элемента хранится [support, node_link].\n",
    "    count: количество повторений транзакции.\n",
    "    \"\"\"\n",
    "    if len(items) == 0:\n",
    "        return\n",
    "    first_item = items[0]\n",
    "    if first_item in node.children:\n",
    "        node.children[first_item].increment(count)\n",
    "    else:\n",
    "        new_node = FPNode(first_item, count, node)\n",
    "        node.children[first_item] = new_node\n",
    "        if header_table[first_item][1] is None:\n",
    "            header_table[first_item][1] = new_node\n",
    "        else:\n",
    "            current = header_table[first_item][1]\n",
    "            while current.node_link is not None:\n",
    "                current = current.node_link\n",
    "            current.node_link = new_node\n",
    "    update_tree(items[1:], node.children[first_item], header_table, count)\n",
    "\n",
    "def create_fp_tree(transactions, min_support):\n",
    "    \"\"\"\n",
    "    Строит FP-дерево из списка транзакций.\n",
    "    transactions: список транзакций (каждая транзакция – список элементов).\n",
    "    min_support: абсолютный порог поддержки (число транзакций).\n",
    "    Возвращает: (root FPNode, header_table).\n",
    "    \"\"\"\n",
    "    header_table = {}\n",
    "    # Первый проход: подсчет поддержки каждого айтема.\n",
    "    for trans in transactions:\n",
    "        for item in trans:\n",
    "            header_table[item] = header_table.get(item, 0) + 1\n",
    "    header_table = {item: [support, None] for item, support in header_table.items() if support >= min_support}\n",
    "    freq_items = set(header_table.keys())\n",
    "    if len(freq_items) == 0:\n",
    "        return None, None\n",
    "    root = FPNode('Null', 1, None)\n",
    "    # Второй проход: построение FP-дерева.\n",
    "    for trans in transactions:\n",
    "        trans_items = [item for item in trans if item in freq_items]\n",
    "        trans_items.sort(key=lambda item: header_table[item][0], reverse=True)\n",
    "        if len(trans_items) > 0:\n",
    "            update_tree(trans_items, root, header_table, 1)\n",
    "    return root, header_table\n",
    "\n",
    "def find_prefix_paths(base_item, header_table):\n",
    "    \"\"\"\n",
    "    Находит условную базу паттернов для base_item.\n",
    "    Возвращает: словарь {path: count}.\n",
    "    \"\"\"\n",
    "    paths = {}\n",
    "    node = header_table[base_item][1]\n",
    "    while node is not None:\n",
    "        prefix_path = []\n",
    "        parent = node.parent\n",
    "        while parent is not None and parent.item != 'Null':\n",
    "            prefix_path.append(parent.item)\n",
    "            parent = parent.parent\n",
    "        if len(prefix_path) > 0:\n",
    "            paths[frozenset(prefix_path)] = paths.get(frozenset(prefix_path), 0) + node.count\n",
    "        node = node.node_link\n",
    "    return paths\n",
    "\n",
    "def create_conditional_fp_tree(prefix_paths, min_support):\n",
    "    \n",
    "    #Создает условное FP-дерево на основе агрегированных prefix_paths.\n",
    "    #prefix_paths: словарь {frozenset(path): count}.\n",
    "    #min_support: порог поддержки.\n",
    "    #Возвращает: (conditional_tree, conditional_header).\n",
    "\n",
    "    freq = {}\n",
    "    for path, count in prefix_paths.items():\n",
    "        for item in path:\n",
    "            freq[item] = freq.get(item, 0) + count\n",
    "    freq = {item: sup for item, sup in freq.items() if sup >= min_support}\n",
    "    if len(freq) == 0:\n",
    "        return None, None\n",
    "    conditional_header = {item: [sup, None] for item, sup in freq.items()}\n",
    "    root = FPNode('Null', 1, None)\n",
    "    for path, count in prefix_paths.items():\n",
    "        filtered_items = [item for item in path if item in freq]\n",
    "        filtered_items.sort(key=lambda item: freq[item], reverse=True)\n",
    "        if len(filtered_items) > 0:\n",
    "            update_tree(filtered_items, root, conditional_header, count)\n",
    "    return root, conditional_header     \n",
    "def mine_fp_tree(fp_tree, header_table, min_support, pre_fix, freq_itemsets):\n",
    "\n",
    "    #Рекурсивно добывает частые наборы из FP-дерева.\n",
    "    #pre_fix: текущий префикс (множество).\n",
    "    #freq_itemsets: словарь, куда записываются найденные частые наборы с их поддержкой.\n",
    "\n",
    "        sorted_items = sorted(header_table.items(), key=lambda p: p[1][0])\n",
    "        for base_item, item_info in sorted_items:\n",
    "            new_freq_set = pre_fix.copy()\n",
    "            new_freq_set.add(base_item)\n",
    "            freq_itemsets[frozenset(new_freq_set)] = item_info[0]\n",
    "            prefix_paths = find_prefix_paths(base_item, header_table)\n",
    "            conditional_tree, conditional_header = create_conditional_fp_tree(prefix_paths, min_support)\n",
    "            if conditional_header is not None:\n",
    "                mine_fp_tree(conditional_tree, conditional_header, min_support, new_freq_set, freq_itemsets)\n",
    "\n",
    "##############################################\n",
    "# 4. Генерация ассоциативных правил (A -> Label)\n",
    "##############################################\n",
    "\n",
    "def powerset(s):\n",
    "    \"\"\"\n",
    "    Возвращает все непустые подмножества множества s (кроме полного множества).\n",
    "    \"\"\"\n",
    "    s = list(s)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)))\n",
    "\n",
    "def generate_association_rules(freq_itemsets, min_conf):\n",
    "    \"\"\"\n",
    "    Генерирует ассоциативные правила вида A -> B, где B содержит только элементы, начинающиеся с \"Label:\".\n",
    "    freq_itemsets: словарь {frozenset(itemset): support}.\n",
    "    min_conf: минимальное значение уверенности.\n",
    "    Возвращает список правил: каждый элемент – кортеж (A, B, support, confidence).\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    for itemset in freq_itemsets:\n",
    "        if len(itemset) < 2:\n",
    "            continue\n",
    "        for subset in powerset(itemset):\n",
    "            A = frozenset(subset)\n",
    "            B = itemset.difference(A)\n",
    "            if len(B) > 0 and all(item.startswith(\"Label:\") for item in B):\n",
    "                support_itemset = freq_itemsets[itemset]\n",
    "                support_A = freq_itemsets.get(A, 0)\n",
    "                if support_A > 0:\n",
    "                    confidence = support_itemset / support_A\n",
    "                    if confidence >= min_conf:\n",
    "                        rules.append((A, B, support_itemset, confidence))\n",
    "    return rules\n",
    "##############################################\n",
    "# 5. Основной блок: чтение данных, предобработка, построение транзакций и запуск алгоритма\n",
    "##############################################\n",
    "def main():\n",
    "    # Чтение данных (пример для одного файла CICIDS2017)\n",
    "    filename = r'C:\\Users\\Гребенников Матвей\\Desktop\\Диплом\\Диплом\\Code\\ML&TEST\\merged_sampled.csv'  # Замените на путь к вашему файлу\n",
    "    df = pd.read_csv(filename, low_memory=False)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Отбор признаков. Пример: Protocol, Flow Duration, Flow Bytes/s, Destination Port, Source Port, Label.\n",
    "    selected_columns = [ 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'Active Mean', 'Idle Mean', 'Label']\n",
    "    \n",
    "    # Предобработка: дискретизация и преобразование значений\n",
    "    df_proc = preprocess_data(df, selected_columns)\n",
    "    \n",
    "    # Преобразование в транзакции (каждая строка – список айтемов)\n",
    "    transactions = transactions_from_df(df_proc)\n",
    "    print(f\"Общее число транзакций: {len(transactions)}\")\n",
    "    \n",
    "    # Построение FP-дерева\n",
    "    min_support = 10  # Порог поддержки (абсолютное число транзакций)\n",
    "    fp_tree, header_table = create_fp_tree(transactions, min_support)\n",
    "    if fp_tree is None:\n",
    "        print(\"Нет частых элементов, удовлетворяющих min_support.\")\n",
    "        return\n",
    "    \n",
    "    # Добыча частых наборов (FP-growth)\n",
    "    freq_itemsets = {}\n",
    "    mine_fp_tree(fp_tree, header_table, min_support, set(), freq_itemsets)\n",
    "    print(f\"Найдено {len(freq_itemsets)} частых наборов\")\n",
    "    \n",
    "    # Сохранение частых наборов в CSV файл\n",
    "    freq_data = []\n",
    "    for itemset, support in freq_itemsets.items():\n",
    "        freq_data.append({\n",
    "            'itemset': ','.join(sorted(itemset)),\n",
    "            'support': support\n",
    "        })\n",
    "    freq_df = pd.DataFrame(freq_data)\n",
    "    freq_df.to_csv(r'C:\\Users\\Гребенников Матвей\\Desktop\\Диплом\\Диплом\\Code\\diplom-project\\diplom\\result\\Deep\\frequent_itemsets.csv', index=False)\n",
    "    print(\"Частые наборы сохранены в frequent_itemsets.csv\")\n",
    "    \n",
    "    # Генерация ассоциативных правил вида (A) -> Label\n",
    "    min_conf = 0.8  # Порог уверенности\n",
    "    rules = generate_association_rules(freq_itemsets, min_conf)\n",
    "    print(f\"Найдено {len(rules)} ассоциативных правил с min_conf = {min_conf}\")\n",
    "    \n",
    "    # Вывод всех найденных правил\n",
    "    rules_data = []\n",
    "    for A, B, support, conf in rules:\n",
    "        rules_data.append({\n",
    "            'A': ','.join(sorted(A)),\n",
    "            'B': ','.join(sorted(B)),\n",
    "            'support': support,\n",
    "            'confidence': conf\n",
    "        })\n",
    "    rules_df.to_csv(r'C:\\Users\\Гребенников Матвей\\Desktop\\Диплом\\Диплом\\Code\\diplom-project\\diplom\\result\\Deep\\association_rules.csv', index=False)\n",
    "    print(\"Ассоциативные правила сохранены в association_rules.csv\")\n",
    "    \n",
    "    # Вывод всех правил в консоль\n",
    "    for rule in rules_data:\n",
    "        print(rule)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
